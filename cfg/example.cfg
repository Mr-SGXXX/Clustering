[global]
run_times               = 10
seed                    = None
# seed                    = 2025
device                  = cuda:0
# description of this experiment, will be used as the name of the log file and figure dir
# when None, use default format: method_name_dataset_name_time_stamp
description             = None 
log_std_output          = False

# dataset setting
# Image Datasets
# dataset                 = MNIST
# dataset                 = FashionMNIST
# dataset                 = USPS
# dataset                 = CIFAR10
# dataset                 = CIFAR100
# dataset                 = STL10
# dataset                 = ImageNet-10
# dataset                 = ImageNet-Dogs
# dataset                 = ImageNet

# Seq Datasets
# dataset                 = Reuters10K
# dataset                 = XYh5_scRNA
# 
# Graph Datasets
dataset                 = Cora  
# dataset                 = Citeseer
# dataset                 = Pubmed   
# dataset                 = CoraFull
# dataset                 = EAT
# dataset                 = BAT
# dataset                 = UAT
# dataset                 = ACM
# dataset                 = DBLP
# dataset                 = Wiki
# dataset                 = Amazon_Computers
# dataset                 = Amazon_Photo
# dataset                 = Coauthor_CS
# dataset                 = Coauthor_Physics
# dataset                 = obgn_arxiv
# dataset                 = obgn_papers100M
# dataset                 = obgn_products



# method setting

# Classical Clustering Methods
# method_name             = KMeans
# method_name             = SpectralClustering
# method_name             = AGC

# Deep Clustering Methods
# method_name             = IDEC
# method_name             = DEC
# method_name             = DeepCluster
# method_name             = CC
# method_name             = IIC
# method_name             = EDESC
# method_name             = DivClust

# Deep graph clustering methods
# method_name             = node2vec
# method_name             = GRACE_2017
# method_name             = DiffPool
# method_name             = DGI
# method_name             = GRACE_2020
method_name             = MVGRL
# method_name             = SDCN
# method_name             = MinCutPool
# method_name             = DFCN
# method_name             = DCRN
# method_name             = DMoN
# method_name             = S3GC
# method_name             = MGCN
# method_name             = DGCluster
# method_name             = MAGI

# Demo
# method_name             = Graph1


# directory setting
log_dir                 = ./log
dataset_dir             = ./data
weight_dir              = ./weight
result_dir              = ./result
figure_dir              = ./figures

# "None" means no email reminder
email_cfg_path          = None
# email_cfg_path          = ./cfg/email.cfg 

# pyerm experiment record database path, None means not use.
pyerm_db_path           = ./result/MAGI.db
# pyerm_db_path           = ./result/recorder1.db
# pyerm_db_path           = ./result/recorder.db
# pyerm_db_path           = None


# whether to record the silhouette coefficient 
# it shows the change of the data compactness and separation during training, but will cost much time when dealing big dataset.
record_sc               = False
# whether to save the experiment result
# save_experiment_result  = True
save_experiment_result  = False
# whether to use ground truth K got from the dataset
use_ground_truth_K      = True
# if use_pretrain_weight is True, the weight file should be in the weight_dir otherwise it will be treat as None
use_pretrain_weight     = True
# if use_ground_truth_K is True, this value will be ignored
n_clusters              = 10
# the number of workers for dataloader, when the dataset is in the memory, set it to 0.
workers                 = 0

# Dataset specific parameters
# image datasets
[MNIST]
img2seq_method          = resnet50
# img2seq_method          = flatten

[FashionMNIST]
img2seq_method          = resnet50
# img2seq_method          = flatten

[USPS]
img2seq_method          = resnet50
# img2seq_method          = flatten

[CIFAR10]
img2seq_method          = resnet50
# img2seq_method          = flatten

[CIFAR100]
super_class             = True
img2seq_method          = resnet50
# img2seq_method          = flatten

[STL10]
# img2seq_method          = resnet50
# img2seq_method          = flatten
img2seq_method          = hog_color

[ImageNet-10]
ImageNet_dir            = None
img2seq_method          = resnet50
# img2seq_method          = flatten

[ImageNet-Dogs]
ImageNet_dir            = None
img2seq_method          = resnet50
# img2seq_method          = flatten

# seq datasets
[Reuters10K]
# None

[XYh5_scRNA]
data_name               = Baron_human
copy                    = True
highly_genes            = 1000
size_factors            = True
normalize_input         = True
logtrans_input          = True
SVD_impute              = True

# graph datasets
[Cora]
# None
[Citeseer]
# None
[Pubmed]
# None
[CoraFull]
# None
[ACM]
# None
[DBLP]
# None
[Wiki]
# None
[BAT]
# None
[EAT]
# None
[UAT]
# None
[Amazon_Computers]
# None
[Amazon_Photo]
# None
[Coauthor_CS]
# None
[Coauthor_Physics]
# None
[Reddit]
# None
[obgn_arxiv]
# None
[obgn_products]
# None
[obgn_papers100M]
# None

# classical clustering methods
[KMeans]
max_iterations          = 100
# batch_size=-1 means use the whole dataset
# batch_size              = -1 
batch_size              = 100000

[SpectralClustering]
# cut_type                = RatioCut
cut_type                = NCut

distance_type           = nearest_neighbors
# distance_type           = euclidean
# distance_type           = cosine

[AGC]
max_iterations          = 60

# deep clustering methods
[EDESC]

learn_rate              = 0.001
batch_size              = 256
d                       = 5
eta                     = 5
beta                    = 0.1
encoder_dims            = 500, 500, 1000
decoder_dims            = 1000, 500, 500
pretrain_file           = None

# use the official pretrain weight
# pretrain_file           = EDESC_reuters.pkl

[DeepCluster]
arch                    = alexnet
# arch                    = vgg16
sobel                   = True
clustering              = Kmeans
# clustering              = PIC
learn_rate              = 0.05
weight_decay            = -5
reassign                = 1
epochs                  = 200  
batch_size              = 256
momentum                = 0.9
# checkpoint file name, locate in `weight_dir`
# `download` means use the checkpoint offered by author. None means train from start
resume                  = download
# resume                  = None
checkpoint_freq         = 25000

[CC]
batch_size              = 256
image_size              = 224
# checkpoint file name, locate in `weight_dir`
# None means train from start
resume                  = None
epochs                  = 1000
# ResNet18, ResNet34, ResNet50
resnet                  = ResNet34
feature_dim             = 128
learn_rate              = 0.0003
weight_decay            = 0.
instance_temperature    = 0.5
cluster_temperature     = 1.0
 
[DEC]
pretrain_file           = None

# the pretrain weight from official IDEC, `https://github.com/XifengGuo/data-and-models/tree/master/ae_weights`
# pretrain_file           = DEC_reutersidf10k_ae_weights.h5
# pretrain_file           = DEC_usps_ae_weights.h5
# pretrain_file           = DEC_mnist_ae_weights.h5
pretrain_learn_rate     = 0.1
layer_wise_pretrain     = False
# layer_wise_pretrain     = True
learn_rate              = 0.01
momentum                = 0.9
batch_size              = 256
train_max_epoch         = 200
alpha                   = 1.0
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[IDEC]
pretrain_file           = None

# the pretrain weight from official IDEC, `https://github.com/XifengGuo/data-and-models/tree/master/ae_weights`
# pretrain_file           = DEC_reutersidf10k_ae_weights.h5
# pretrain_file           = DEC_usps_ae_weights.h5
# pretrain_file           = DEC_mnist_ae_weights.h5

pretrain_learn_rate     = 0.1
# layer_wise_pretrain     = True
layer_wise_pretrain     = False
learn_rate              = 0.1
# update_interval 3 for Reuters10K, 30 for usps, 140 for mnist
update_interval         = 3
momentum                = 0.99
batch_size              = 256
train_max_epoch         = 200
alpha                   = 1.0
gamma                   = 0.1
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[node2vec]
dimensions              = 128
walk_length             = 80
num_walks               = 10
window_size             = 10
iter                    = 1
p                       = 1
q                       = 1
directed                = False

[GRACE_2017]
encoder_dims            = 256
decoder_dims            = 256
embedding_dim           = 256
transition_function     = RI
random_walk_step        = 2
alpha                   = 0.9
lambda_                 = 0.1
dropout_rate            = 0.5
bn_flag                 = False
lambda_r                = 1.0
lambda_c                = 0.2
learn_rate              = 1e-3
pre_epoch               = 1000
epoch                   = 30
step                    = 30
epsilon                 = 1.0

[DiffPool]
# hidden_dims             = 64
hidden_dims             = 512
learn_rate              = 0.001
num_epochs              = 1000

[DGI]
nb_epochs               = 10000
patience                = 20
learn_rate              = 0.001
l2_coef                 = 0.0
hid_units               = 512
nonlinearity            = prelu

[GRACE_2020]
learn_rate           = 0.0005
num_hidden              = 128
num_proj_hidden         = 128
activation              = relu
base_model              = GCNConv
num_layers              = 2
drop_edge_rate_1        = 0.2
drop_edge_rate_2        = 0.4
drop_feature_rate_1     = 0.3
drop_feature_rate_2     = 0.4
tau                     = 0.4
num_epochs              = 200
weight_decay            = 0.00001

[MVGRL]
nb_epochs               = 3000
patience                = 20
learn_rate              = 0.001
l2_coef                 = 0.0
hid_units               = 512
sample_size             = 2000
batch_size              = 4
diffusion_type          = ppr 

[SDCN]
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
pretrain_batch_size     = 256
train_max_epoch         = 200
alpha                   = 1.0
sigma                   = 0.5
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[DFCN]
input_dim               = 50
# input_dim               = 100
lambda_value            = 10
gamma_value             = 0.1
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
# learn_rate              = 1e-4
freedom_degree          = 1.0
epochs                  = 200

[MinCutPool]
# hidden_dims             = 64
hidden_dims             = 512
learn_rate              = 0.001
orthogonality_weight    = 1.0
num_epochs              = 1000

[DMoN]
# hidden_dims             = 64
hidden_dims             = 512
learn_rate              = 0.001
collapse_weight         = 1.0
dropout_rate            = 0.5
# dropout_rate            = 0.0
num_epochs              = 1000

[DCRN]
input_dim               = 50
alpha_value             = 0.2
lambda_value            = 10
gamma_value             = 1e3
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
# learn_rate              = 1e-4
freedom_degree          = 1.0
epochs                  = 400

[S3GC]
# hidden_dims             = 256
hidden_dims             = 512
walk_length             = 3
walks_per_node          = 10
batch_size              = 4096
learn_rate              = 0.01
epochs                  = 50
# epochs                  = 200
big_model               = False
# big_model               = True

[MGCN]
# input_dim               = 50
input_dim               = 100
loss_w                  = 0.1
loss_a                  = 0.1
loss_s                  = 0.1
loss_kl                 = 10
n_z                     = 20
sigma                   = 0.7
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
# learn_rate              = 1e-4
# learn_rate              = 5e-5
freedom_degree          = 1.0
epochs                  = 200



[DGCluster]
base_model              = GCN
# base_model              = GAT
# base_model              = GIN
# base_model              = GraphSAGE
lam                     = 0.2
# lam                     = 0.8
alp                     = 0.0
epochs                  = 300
# ground_truth_for_train  = True
ground_truth_for_train  = False

pre_clustering_method   = Louvain
# pre_clustering_method   = KMeans



# This part should be in another file located by the `email_cfg_path` in the section `global`
# whether to use email reminder
[email_reminder] 
in_use                  = False 
mail_host               = your_mail_host
mail_user               = your_mail_name
mail_pwd                = your_mail_password
receivers               = mails_to_receive
sender                  = your_mail_address
# max size of the attachment in MB
max_size_mb             = 20 