[global]
seed                    = None
device                  = cuda:0
# description of this experiment, will be used as the name of the log file and figure dir
# when None, use default format: method_name_dataset_name_time_stamp
description             = None 
log_std_output          = False

# dataset setting
# Image Datasets
# dataset                 = MNIST
# dataset                 = FashionMNIST
dataset                 = USPS
# dataset                 = CIFAR10
# dataset                 = CIFAR100
# dataset                 = STL10
# dataset                 = ImageNet-10
# dataset                 = ImageNet-Dogs
# dataset                 = ImageNet

# Seq Datasets
# dataset                 = Reuters10K
# dataset                 = XYh5_scRNA
# 
# Graph Datasets
# dataset                 = Cora  
# dataset                 = Citeseer
# dataset                 = Pubmed
# dataset                 = CoraFull
# dataset                 = ACM
# dataset                 = DBLP
# dataset                 = AMAP
# dataset                 = Wiki
# dataset                 = Amazon_Computers
# dataset                 = Amazon_Photo
# dataset                 = Coauthor_CS
# dataset                 = Coauthor_Physics
# dataset                 = obgn_arxiv
# dataset                 = obgn_papers100M
# dataset                 = obgn_products



# method setting

# Classical Clustering Methods
# method_name             = KMeans
# method_name             = SpectralClustering
# method_name             = AGC

# Deep Clustering Methods
# method_name             = IDEC
# method_name             = DEC
# method_name             = DeepCluster
# method_name             = CC
# method_name             = IIC
# method_name             = EDESC
# method_name             = DivClust

# Deep graph clustering methods
# method_name             = SDCN
# method_name             = MinCutPool
# method_name             = DFCN
# method_name             = DCRN
# method_name             = DMoN
method_name             = DGCluster
# method_name             = MAGI



# directory setting
log_dir                 = ./log
dataset_dir             = ./data
weight_dir              = ./weight
result_dir              = ./result
figure_dir              = ./figures

# "None" means no email reminder
email_cfg_path          = None
# email_cfg_path          = ./cfg/email.cfg 

# pyerm experiment record database path, None means not use.
# pyerm_db_path           = ./result/recorder1.db
pyerm_db_path           = ./result/recorder.db
# pyerm_db_path           = None


# whether to record the silhouette coefficient 
# it shows the change of the data compactness and separation during training, but will cost much time when dealing big dataset.
record_sc               = False
# whether to save the experiment result
# save_experiment_result  = True
save_experiment_result  = False
# whether to use ground truth K got from the dataset
use_ground_truth_K      = True
# if use_pretrain_weight is True, the weight file should be in the weight_dir otherwise it will be treat as None
use_pretrain_weight     = True
# if use_ground_truth_K is True, this value will be ignored
n_clusters              = 10
# the number of workers for dataloader, when the dataset is in the memory, set it to 0.
workers                 = 0

# Dataset specific parameters
# image datasets
[MNIST]
img2seq_method          = resnet50
# img2seq_method          = flatten

[FashionMNIST]
img2seq_method          = resnet50
# img2seq_method          = flatten

[USPS]
img2seq_method          = resnet50
# img2seq_method          = flatten

[CIFAR10]
img2seq_method          = resnet50
# img2seq_method          = flatten

[CIFAR100]
super_class             = True
img2seq_method          = resnet50
# img2seq_method          = flatten

[STL10]
# img2seq_method          = resnet50
# img2seq_method          = flatten
img2seq_method          = hog_color

[ImageNet-10]
ImageNet_dir            = None
img2seq_method          = resnet50
# img2seq_method          = flatten

[ImageNet-Dogs]
ImageNet_dir            = None
img2seq_method          = resnet50
# img2seq_method          = flatten

# seq datasets
[Reuters10K]
# None

[XYh5_scRNA]
data_name               = Baron_human
copy                    = True
highly_genes            = 1000
size_factors            = True
normalize_input         = True
logtrans_input          = True
SVD_impute              = True

# graph datasets
[Cora]
# None
[Citeseer]
# None
[Pubmed]
# None
[CoraFull]
# None
[ACM]
# None
[DBLP]
# None
[AMAP]
# None
[Wiki]
# None
[Amazon_Computers]
# None
[Amazon_Photo]
# None
[obgn_arxiv]
# None
[obgn_products]
# None
[obgn_papers100M]
# None

# classical clustering methods
[KMeans]
max_iterations          = 100
# batch_size=-1 means use the whole dataset
# batch_size              = -1 
batch_size              = 100000

[SpectralClustering]
# cut_type                = RatioCut
cut_type                = NCut

distance_type           = nearest_neighbors
# distance_type           = euclidean
# distance_type           = cosine

[AGC]
max_iterations          = 60

# deep clustering methods
[EDESC]

learn_rate              = 0.001
batch_size              = 256
d                       = 5
eta                     = 5
beta                    = 0.1
encoder_dims            = 500, 500, 1000
decoder_dims            = 1000, 500, 500
pretrain_file           = None

# use the official pretrain weight
# pretrain_file           = EDESC_reuters.pkl

[DeepCluster]
arch                    = alexnet
# arch                    = vgg16
sobel                   = True
clustering              = Kmeans
# clustering              = PIC
learn_rate              = 0.05
weight_decay            = -5
reassign                = 1
epochs                  = 200  
batch_size              = 256
momentum                = 0.9
# checkpoint file name, locate in `weight_dir`
# `download` means use the checkpoint offered by author. None means train from start
resume                  = download
# resume                  = None
checkpoint_freq         = 25000

[CC]
batch_size              = 256
image_size              = 224
# checkpoint file name, locate in `weight_dir`
# None means train from start
resume                  = None
epochs                  = 1000
# ResNet18, ResNet34, ResNet50
resnet                  = ResNet34
feature_dim             = 128
learn_rate              = 0.0003
weight_decay            = 0.
instance_temperature    = 0.5
cluster_temperature     = 1.0
 
[DEC]
pretrain_file           = None

# the pretrain weight from official IDEC, `https://github.com/XifengGuo/data-and-models/tree/master/ae_weights`
# pretrain_file           = DEC_reutersidf10k_ae_weights.h5
# pretrain_file           = DEC_usps_ae_weights.h5
# pretrain_file           = DEC_mnist_ae_weights.h5
pretrain_learn_rate     = 0.1
layer_wise_pretrain     = False
# layer_wise_pretrain     = True
learn_rate              = 0.01
momentum                = 0.9
batch_size              = 256
train_max_epoch         = 200
alpha                   = 1.0
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[IDEC]
pretrain_file           = None

# the pretrain weight from official IDEC, `https://github.com/XifengGuo/data-and-models/tree/master/ae_weights`
# pretrain_file           = DEC_reutersidf10k_ae_weights.h5
# pretrain_file           = DEC_usps_ae_weights.h5
# pretrain_file           = DEC_mnist_ae_weights.h5

pretrain_learn_rate     = 0.1
# layer_wise_pretrain     = True
layer_wise_pretrain     = False
learn_rate              = 0.1
# update_interval 3 for Reuters10K, 30 for usps, 140 for mnist
update_interval         = 3
momentum                = 0.99
batch_size              = 256
train_max_epoch         = 200
alpha                   = 1.0
gamma                   = 0.1
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[SDCN]
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
pretrain_batch_size     = 256
train_max_epoch         = 200
alpha                   = 1.0
sigma                   = 0.5
hidden_dim              = 10
encoder_dims            = 500, 500, 2000
tol                     = 1e-3

[DFCN]
input_dim               = 50
lambda_value            = 10
gamma_value             = 0.1
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
freedom_degree          = 1.0
epochs                  = 200

[MinCutPool]
hidden_dims             = 64
learn_rate              = 0.001
orthogonality_weight    = 1.0
num_epochs              = 1000

[DMoN]
hidden_dims             = 64
learn_rate              = 0.001
collapse_weight         = 1.0
dropout_rate            = 0.0
num_epochs              = 1000

[DCRN]
input_dim               = 50
alpha_value             = 0.2
lambda_value            = 10
gamma_value             = 1e3
pretrain_learn_rate     = 1e-3
learn_rate              = 1e-3
# learn_rate              = 1e-4
freedom_degree          = 1.0
epochs                  = 400

[MAGI]
base_model              = GCN
hidden_dims             = 512
wt                      = 100
wl                      = 2
tau                     = 0.3
dropout                 = 0.1
learn_rate              = 0.0005
weight_decay            = 1e-3
epochs                  = 400 
clustering_method       = SpectralClustering


# base_model              = GraphSAGE
# hidden_dims             = 512, 256
# wt                      = 20
# wl                      = 4
# tau                     = 0.5
# dropout                 = 0
# learn_rate              = 0.01
# epochs                  = 100
# clustering_method       = KMeans

kmeans_batch            = -1
batch_size              = 2048
projection_dims         = None
sizes_neighbor          = 10, 10
negative_slope          = 0.5

[DGCluster]
base_model              = GCN
# base_model              = GAT
# base_model              = GIN
# base_model              = GraphSAGE
# lam                     = 0.2
lam                     = 0.8
alp                     = 0.0
epochs                  = 300
ground_truth_for_train  = True
# ground_truth_for_train  = False

pre_clustering_method   = Louvain
# pre_clustering_method   = KMeans


[email_reminder] 
# This part should be in another file located by the `email_cfg_path` in the section `global`
# whether to use email reminder
in_use                  = False 
mail_host               = your_mail_host
mail_user               = your_mail_name
mail_pwd                = your_mail_password
receivers               = mails_to_receive
sender                  = your_mail_address
# max size of the attachment in MB
max_size_mb             = 20 